{"name":"Pml-assn","tagline":"Practical Machine Learning assn WriteUp","body":"---\r\ntitle: \"Practical Machine Learning-Data Science Specialization\"\r\nauthor: \"Coursera Student\"\r\ndate: \"21 August 2014\"\r\noutput: html_document\r\n---\r\n```{r setup, include=FALSE}\r\nknitr::opts_chunk$set(cache=TRUE)\r\n```\r\n```{r reading data, echo = FALSE}\r\nload(\"training.RData\")\r\nload(\"test.RData\")\r\n```\r\n# PREPROCESSING DATA\r\n```{r exlporation 0}\r\ndim(training)\r\ndim(test)\r\n```\r\n#### Are variable names unique?\r\n```{r exlporation 1}\r\nsum(table(names(training)))\r\nsum(table(unique(names(training))))\r\nsum(table(names(test)))\r\nsum(table(unique(names(test))))\r\n```\r\n#### Do training and test variable names match?\r\n```{r exlporation 2}\r\ntable(names(training)==names(test))\r\nnames(training)[names(training)!=names(test)]\r\nnames(test)[names(test)!=names(training)]\r\n``` \r\n#### Are there missing values?\r\n```{r exlporation 3}\r\ngetNa <- function(dfrm) lapply(dfrm, function(x) length(which(is.na(x) ==\"TRUE\")))\r\ng <- getNa(training)\r\n# g[g!=0]\r\n# length(g[g!=0])\r\ng1 <- getNa(test)\r\n# g1[g1!=0]\r\n# length(g1[g1!=0])\r\n```  \r\nThere are `r length(g[g!=0])` variables with missing values in the training set. Exactly 19216 rows are missing values from all of these variables.\r\nThere are `r length(g1[g1!=0])` variables with missing values in the test set. All 20 rows are missing values from all of these variables.\r\n\r\n#### Do names of variables missing in training and test tally?\r\n```{r exlporation 4, eval = FALSE}\r\ntable(names(g1[g1!=0]) %in% names(g[g!=0]))\r\n``` \r\nDecided to remove all 100 variables from analyses. 67 variables are missing in all or\r\nmajority of cases in both training and test. If there was more data in these, imputation would be attempted. Remaining 33 variables that are missing in test cannot be included since predictions cannot be made on test with these.\r\n\r\n#### Creating new training set by removing variables with missing values.\r\n#### Removing first column containing row numbers and last with outcome, \"classe\".\r\n#### Converting \"user_name\" and \"cvtd_timestamp\" to numeric factors\r\n```{r exlporation 5}\r\nn <- names(g1[g1!=0])\r\nremnames <- names(training)[!(names(training) %in% n)]\r\nnewtrain <- training[,intersect(names(training),remnames)]\r\ndim(newtrain)\r\nnewtrain <- newtrain[,-c(1,60)]\r\nfac <- newtrain[,c(1,4)]\r\nfac[,1] <- as.numeric(fac[,1])\r\nfac[,2] <- as.numeric(fac[,2])\r\nfac[,1] <- as.factor(fac[,1])\r\nfac[,2] <- as.factor(fac[,2])\r\nsummary(fac)\r\nnewtrain <- newtrain[,-c(1,4)]\r\nnewtrain <- cbind(fac, newtrain)\r\ndim(newtrain)\r\n```  \r\nThe training and test data are from `r length(unique(newtrain[,2]))` people.\r\n\r\n#### Filtering out variables with near zero variance\r\n```{r exlporation 6, warning=FALSE}\r\nlibrary(caret)\r\nnzv <- nearZeroVar(newtrain)\r\nfiltnewtrain <- newtrain[,-nzv]\r\ndim(filtnewtrain)\r\n```  \r\nThe variable `r setdiff(names(newtrain), names(filtnewtrain))` is removed by the near zero variance filter of caret package.\r\n\r\n#### Converting data frame to numeric matrix \r\n#### Applying correlation filter - remove variables with correlation > 0.90\r\n```{r exploration 7}\r\nfil <- sapply(filtnewtrain, as.numeric)\r\ncorfil <- cor(fil)\r\nf <- findCorrelation(corfil)\r\nfil2 <- fil[,-f]\r\nvars_to_remove <- setdiff(colnames(fil), colnames(fil2))\r\n```\r\nVariables \"vars_to_remove\" were removed due to high correlation.  \r\nNo linear combinations were found using appropriate function in caret package.\r\n\r\n#### Subsetting same variables in test data and making new test set\r\n```{r new test}\r\nfiltest <- test[,names(test)%in% colnames(fil2)]\r\ndim(filtest)\r\nfiltest <- sapply(filtest, as.numeric)\r\n```\r\n\r\n#### Centering and scaling training and test data after removing \"user_name\" variable\r\n#### Adding back \"user_name\" for both and \"classe\" for training data\r\n#### Converting to data frame; \"user_name\", \"classe\" and \"cvtd_timestamp\" to factors\r\n```{r preProcess}\r\npreProcVal <- preProcess(fil2[,-1], method=c(\"center\", \"scale\"))\r\ntrainF <- predict(preProcVal, fil2[,-1])\r\ntrainF <- cbind(fil2[,1], trainF)\r\ncolnames(trainF)[1:2] <- c(\"classe\", \"user_name\")\r\ntrainF <- data.frame(trainF)\r\ntrainF[,1] <- as.factor(as.character(trainF[,1]))\r\ntrainF[,2] <- as.factor(as.character(trainF[,2]))\r\ndate <- strptime(training$cvtd_timestamp, \"%d/%m/%Y %H:%M\")\r\ndate <- as.factor(as.character(date))\r\ntrainF[,3] <- date\r\ntestF <- predict(preProcVal, filtest[,-1])\r\ntestF <- cbind(filtest[,1], testF)\r\ncolnames(testF)[1] <- \"user_name\"\r\ntestF <- data.frame(testF)\r\ntestF[,1] <- as.factor(as.character(testF[,1]))\r\ndate <- strptime(test$cvtd_timestamp, \"%d/%m/%Y %H:%M\")\r\ndate <- as.factor(as.character(date))\r\ntestF[,4] <- date\r\n```\r\n## EXPLORATORY ANALYSIS\r\n\r\n library(caret)\r\n\r\n#### Timestamp\r\n```{r timestamp}\r\nfeaturePlot( x = trainF[,4:5],\r\n             y = trainF$classe,\r\n             plot = \"density\",\r\n            auto.key = list(columns = 2))\r\n```\r\n\r\n#### Variables related to belt and arm\r\n```{r box1}\r\nfeaturePlot( x = trainF[,6:27],\r\n             y = trainF$classe,\r\n             plot = \"box\",\r\n             auto.key = list(columns = 2))\r\n```\r\n\r\n#### Variables related to dumbell\r\n```{r box2}\r\nfeaturePlot( x = trainF[,c(28:31, 34:39)],\r\n             y = trainF$classe,\r\n             plot = \"box\",\r\n            auto.key = list(columns = 2))\r\n```                        \r\n\r\n#### Variables related to forearm\r\n```{r box 3}\r\nfeaturePlot( x = trainF[,c(40:43,46:50)],\r\n             y = trainF$classe,\r\n             plot = \"box\",\r\n             auto.key = list(columns = 2))\r\n```\r\n\r\n#### outliers in gyros_forearm_x and gyros_forearm_y, gyros_dumbell_y, gyros_dumbell_z          \r\n```{r box 4}\r\nfeaturePlot( x = trainF[,c(32,33,44,45)],\r\n             y = trainF$classe,\r\n             plot = \"box\",\r\n             auto.key = list(columns = 2))\r\n```\r\n\r\nSingle points are pulling up scale on y-axis to 100. Need to impute these values to median.\r\n\r\n#### Pick out single extreme outliers and replace by median\r\n```{r outlier}\r\nsummary(trainF[,c(32,33,44,45)])\r\nw <- which(trainF$gyros_dumbbell_y > 80)\r\ntrainF$gyros_dumbbell_y[w] <- median(trainF$gyros_dumbbell_y)\r\nw2 <- which(trainF$gyros_dumbbell_z > 100)\r\ntrainF$gyros_dumbbell_z[w2] <- median(trainF$gyros_dumbbell_z)\r\nw3 <- which(abs(trainF$gyros_forearm_x) > 30)\r\ntrainF$gyros_forearm_x[w3] <- median(trainF$gyros_forearm_x)\r\nw4 <- which(trainF$gyros_forearm_y > 99)\r\ntrainF$gyros_forearm_y[w4] <- median(trainF$gyros_forearm_y)\r\nsummary(trainF[,c(32,33,44,45)])\r\n```\r\n\r\n#### replacing numeric with original \"classe\"\r\n```{r classe}\r\ntrainF$classe <- training$classe\r\n```\r\n## BUILDING THE MODEL\r\n\r\n#### Splitting original training set into subtraining and internal test set\r\n```{r split data}\r\nlibrary(caret)\r\nset.seed(1234)\r\nind <- createDataPartition(trainF$classe, p=0.7, list=F)\r\nsubtrain <- trainF[ind,]\r\nsubtest <- trainF[-ind,]\r\n```\r\nChecked density distribution of time variables - structure not changed too much from original\r\n\r\n#### Building the model : Random Forest with crossvalidation\r\n```{r random forest, eval=FALSE}\r\nlibrary(doMC)\r\nregisterDoMC(cores = 5)\r\nfitControl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 10)\r\nset.seed(1234)\r\nrfFit <- train(classe~., data = subtrain, method = \"rf\",\r\ntrControl = fitControl, verbose= FALSE)\r\n````\r\n#### The model object\r\n```\r\nRandom Forest \r\n\r\n13737 samples\r\n   50 predictor\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\nNo pre-processing\r\nResampling: Cross-Validated (10 fold, repeated 10 times) \r\n\r\nSummary of sample sizes: 12365, 12363, 12364, 12364, 12362, 12362, ... \r\n\r\nResampling results across tuning parameters:\r\n\r\n  mtry  Accuracy  Kappa  Accuracy SD  Kappa SD\r\n   2    0.985     0.981  0.003542     0.004484\r\n  37    0.999     0.999  0.000771     0.000975\r\n  72    0.999     0.998  0.000877     0.001109\r\n```\r\nAccuracy was used to select the optimal model using  the largest value.\r\nThe final value used for the model was mtry = 37. \r\n\r\n#### Predicting on held out test set belonging to original training data : \r\n\r\n#### In sample error\r\n```{r insample predict, eval = FALSE}\r\npredRF <- predict(rfFit, newdata = subtest)\r\nconfusionMatrix(predRF, subtest$classe)\r\n```\r\n```\r\nConfusion Matrix and Statistics\r\n\r\n          Reference\r\nPrediction    A    B    C    D    E\r\n         A 1673    1    0    0    0\r\n         B    1 1138    1    0    0\r\n         C    0    0 1025    0    0\r\n         D    0    0    0  964    0\r\n         E    0    0    0    0 1082\r\n\r\nOverall Statistics\r\n                                          \r\n               Accuracy : 0.9995          \r\n                 95% CI : (0.9985, 0.9999)\r\n    No Information Rate : 0.2845          \r\n    P-Value [Acc > NIR] : < 2.2e-16       \r\n                                          \r\n                  Kappa : 0.9994          \r\n Mcnemar's Test P-Value : NA              \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            0.9994   0.9991   0.9990   1.0000   1.0000\r\nSpecificity            0.9998   0.9996   1.0000   1.0000   1.0000\r\nPos Pred Value         0.9994   0.9982   1.0000   1.0000   1.0000\r\nNeg Pred Value         0.9998   0.9998   0.9998   1.0000   1.0000\r\nPrevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\nDetection Rate         0.2843   0.1934   0.1742   0.1638   0.1839\r\nDetection Prevalence   0.2845   0.1937   0.1742   0.1638   0.1839\r\nBalanced Accuracy      0.9996   0.9994   0.9995   1.0000   1.0000\r\n```\r\nThe high accuracy values are indicative of overtraining. Random forest models themselves although powerful are prone to overfitting.  Out of sample prediction cannot be expected to be more accurate than 60-70%. \r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}